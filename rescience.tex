\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}

% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{ReScience~C: a journal for reproducible replications in computational science}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

\author{Nicolas P. Rougier\inst{1,2,3}\orcidID{0000-0002-6972-589X} \and
Konrad Hinsen\inst{4,5}\orcidID{0000-0003-0330-9428}}
%
\authorrunning{N.P. Rougier and K. Hinsen}
%
\institute{INRIA Bordeaux Sud-Ouest Talence, France
\email{Nicolas.Rougier@inria.fr}
\and
Institut des Maladies Neurod\'{e}g\'{e}n\'{e}ratives,
Universit\'{e} de Bordeaux, CNRS UMR 5293, Bordeaux, France
\and
LaBRI, Universit\'{e} de Bordeaux, Bordeaux INP, CNRS UMR 5800, Talence, France
\and
Centre de Biophysique Mol\'{e}culaire, CNRS UPR4301, Orl√©ans, France 
\email{Konrad.Hinsen@cnrs.fr}
\and
Synchrotron SOLEIL, Division Exp\'{e}riences, Gif sur Yvette, France}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  Independent replication is one of the most powerful methods to
  verify published scientific studies.  In computational science, it
  requires the reimplementation of the methods described in the
  original article by a different team of researchers.  Replication is
  often performed by scientists who wish to gain a better
  understanding of a published method, but its results are rarely made
  public. ReScience~C is a peer-reviewed journal dedicated to the
  publication of high-quality computational replications that provide
  added value to the scientific community.  To this end, ReScience~C
  requires replications to be reproducible and implemented using Open
  Source languages and libraries. In this article, we provide an
  overview of ReScience~C's goals and quality standards, outline the
  submission and reviewing processes, and summarize the experience of
  its first three years of operation, concluding with an outlook
  towards evolutions envisaged for the near future.

  \keywords{Open science \and Computational science \and Reproducibility}
\end{abstract}
%
\section{Introduction}

The question of how to attain reliable outcomes from unreliable components pervades many aspects of life. Scientific research is no exception. Individual research contributions are prone to mistakes, and sometimes fraud, and therefore error detection and correction mechanisms are required to reach a higher level of reliability at the collective level. The two main methods for error detection are critical inspection, starting with peer review but continuing well after the publication of an individual contribution, and independent replication of published work. But replication is more than a verification technique. For the researchers performing the replication, it yields a level of understanding and insight that is impossible to achieve by other means. This is in fact the main motivation for much replication work, verification being merely a side effect.

The power but also the limitations of replication as an approach to verification are best illustrated by the recent discussion of replication crises in various scientific domains
\cite{IoannidisWhyMostPublished2005,Baker500scientistslift2016,Munafomanifestoreproduciblescience2017,IqbalReproducibleResearchPractices2016}, which are all based on the observation of frequent failures to replicate published scientific findings. However, a replication failure does not necessarily mean that the original study is flawed as it could well be the replication work that is at fault. More interestingly, both the original and the replication work can be of excellent quality and yet yield different conclusions if some important factor has escaped everyone's attention. In this situation, independent replication can become the first step of completely new lines of research.

Replication is thus an important contribution to science, and its findings should therefore be shared with the scientific community. Unfortunately, most journals do not accept replication studies for publication because originality is one of their selection criteria. For this reason, we launched ReScience  (now called ReScience~C for reasons explained later) in 2015 as a journal dedicated to replications in computational science. In this article, we outline its mode of operation and summarize our experience from the first few years. A more complete account has been published recently \cite{RougierSustainablecomputationalscience2017}

\section{Terminology}

The replication crisis has given rise to an active debate in various domains of science, in which some terms, in particular ``reproducible'' and ``replicable'', are used in different and sometimes opposite meanings. Therefore we start with the definitions that we are using in this article and more generally in ReScience~C. Our definitions are formulated in the specific context of computational science, and are not easily transferable to experimental science \cite{HinsenScientificsoftwaredifferent2018}.

A computation is \textit{reproducible} if the code and input data is available with sufficient instructions for someone else to re-do or \textit{reproduce} the computation. The only point in reproducing the computation is to verify its reproducibility, which in turn is evidence that the archived code and data is (1) complete and (2) indeed the code and data that was used in the original published study. A failed reproduction means that the description of the original code and data is incomplete or inaccurate. A very frequent variety of incompleteness is the lack of a detailed description of the computational environment, i.e. the infrastructure software (operating system, compiler, ...) or code dependencies (libraries, ...) that was used in the original work.

A \textit{replication} of computational work involves writing and then running new software using only the description of a method published in a journal article, without using or consulting the software used by the original authors, which may or may not be available. Successful replication confirms that the method description is complete and accurate, and significantly reduces the probability of an error in either implementation. A failed replication can be caused by such errors or by an inexact or incomplete method description. It requires further investigation which, as explained above, can even lead to new directions of scientific inquiry.

%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{rescience}
%
\end{document}
