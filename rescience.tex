\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}

% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{ReScience~C: a journal for reproducible replications in computational science}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

\author{Nicolas P. Rougier\inst{1,2,3}\orcidID{0000-0002-6972-589X} \and
Konrad Hinsen\inst{4,5}\orcidID{0000-0003-0330-9428}}
%
\authorrunning{N.P. Rougier and K. Hinsen}
%
\institute{INRIA Bordeaux Sud-Ouest Talence, France
\email{Nicolas.Rougier@inria.fr}
\and
Institut des Maladies Neurod\'{e}g\'{e}n\'{e}ratives,
Universit\'{e} de Bordeaux, CNRS UMR 5293, Bordeaux, France
\and
LaBRI, Universit\'{e} de Bordeaux, Bordeaux INP, CNRS UMR 5800, Talence, France
\and
Centre de Biophysique Mol\'{e}culaire, CNRS UPR4301, Orl√©ans, France 
\email{Konrad.Hinsen@cnrs.fr}
\and
Synchrotron SOLEIL, Division Exp\'{e}riences, Gif sur Yvette, France}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  Independent replication is one of the most powerful methods to
  verify published scientific studies.  In computational science, it
  requires the reimplementation of the methods described in the
  original article by a different team of researchers.  Replication is
  often performed by scientists who wish to gain a better
  understanding of a published method, but its results are rarely made
  public. ReScience~C is a peer-reviewed journal dedicated to the
  publication of high-quality computational replications that provide
  added value to the scientific community.  To this end, ReScience~C
  requires replications to be reproducible and implemented using Open
  Source languages and libraries. In this article, we provide an
  overview of ReScience~C's goals and quality standards, outline the
  submission and reviewing processes, and summarize the experience of
  its first three years of operation, concluding with an outlook
  towards evolutions envisaged for the near future.

  \keywords{Open science \and Computational science \and Reproducibility}
\end{abstract}
%
\section{Introduction}

The question of how to attain reliable outcomes from unreliable components pervades many aspects of life. Scientific research is no exception. Individual research contributions are prone to mistakes, and sometimes fraud, and therefore error detection and correction mechanisms are required to reach a higher level of reliability at the collective level. The two main methods for error detection are critical inspection, starting with peer review or article submissions but continuing well after publication, and independent replication of published work. But replication is more than a verification technique. For the researchers performing the replication, it yields a level of understanding and insight that is impossible to achieve by other means. This is in fact the main motivation for much replication work, verification being merely a side effect.

The power but also the limitations of replication as an approach to verification are best illustrated by the recent discussion of replication crises in various scientific domains~\cite{IoannidisWhyMostPublished2005,Baker500scientistslift2016,Munafomanifestoreproduciblescience2017,IqbalReproducibleResearchPractices2016}, which are all based on the observation of frequent failures to replicate published scientific findings. However, a replication failure does not necessarily mean that the original study is flawed. First of all, it could well be the replication work that is at fault. But it is also possible that both the original and the replication work are of excellent quality and yet yield different conclusions, if some important factor has escaped everyone's attention and accidentally differs between the two studies. In this situation, independent replication can become the starting point of completely new lines of research.

Replication is thus an important contribution to science, and its findings should be shared with the scientific community. Unfortunately, most journals do not accept replication studies for publication because originality is one of their selection criteria. For this reason, we launched ReScience (now called ReScience~C for reasons explained later) in 2015 as a journal dedicated to replications of computational research. In this article, we outline its mode of operation and summarize our experience from the first few years. A more complete account, also containing more background references, has been published recently.~\cite{RougierSustainablecomputationalscience2017}

\section{Terminology: reproducible replications}

The replication crisis has given rise to an active debate in various domains of science, in which some terms, in particular ``reproducible'' and ``replicable'', are used with very different meanings. We therefore explain the definitions that we are using in this article and more generally in ReScience~C. Our definitions are formulated in the specific context of computational science, and are not easily transferable to experimental science~\cite{HinsenScientificsoftwaredifferent2018}.

A computation is \textit{reproducible} if the code and input data is available together with sufficient instructions for someone else to re-do or \textit{reproduce} the computation. The only point in reproducing the computation is to verify its reproducibility, which in turn is evidence that the archived code and data is (1) complete and (2) indeed the code and data that was used in the original published study. A failed reproduction means that the description of the original code and data is incomplete or inaccurate. A frequent form of incompleteness is the lack of a detailed description of the computational environment, i.e. the infrastructure software (operating system, compiler, ...) or code dependencies (libraries, ...) that were used in the original work. Reproducible computations are the most detailed and accurate possible description of a computational method within the current state of the art of computational science.

A \textit{replication} of computational work involves writing and then running new software, using only the description of a method published in a journal article, i.e. without using or consulting the software used by the original authors, which may or may not be available. Successful replication confirms that the method description is complete and accurate, and significantly reduces the probability of an error in either implementation. A failed replication can be caused by such errors or by an inexact or incomplete method description. It requires further investigation which, as explained above, can even lead to new directions of scientific inquiry.

A \textit{reproducible replication} is a replication whose code and data has been archived and documented for reproducibility. It is especially useful in the still dominant situation that the target of the replication was not published reproducibly. In that case, the replication provides not only verification, but also the missing code and data.

\section{ReScience~C}

The definition of a replication given above should be sufficient to show that performing replications is a useful activity for a researcher. Moreover, whether successful or not, a replication yields additional insight into the problem that are worth sharing with the scientific community. For example, minor omissions or inaccuracies are inevitable in the narratives that make up for most of a journal article, meaning that replication authors have to do some detective work whose results are of use to others.

Unfortunately, the vast majority of scientific journals would not consider such work for publication, with the possible exception of a failed replication of particularly important findings, because novelty is for them an important selection criterion. Moreover, the reviewing process of traditional scientific journals, designed in the 20th century for experimental and theoretical but not for computational work, cannot handle the technical challenges posed by a verification of reproducibility and successful replication. For these reasons we created the ReScience~C journal (then called simply ReScience) in September~2015 as a state-of-the-art venue for the publication of reproducible replication studies in computational science.

The criteria that a submission must fulfill for acceptance by ReScience~C are the following:
\begin{itemize}
\item It must aim at reproducing all or a significant part of the figures and tables in an already published scientific study.
\item The text of the article must discuss which results were successfully replicated and which, if any, could not be replicated. It should also provide a description of problems that were encountered, e.g. additional assumptions that had to be made.
\item The complete source code of the software used for the replication must be provided, and should have only Open Source software as dependencies in order to allow full inspection of the complete software stack.
\item In order to ensure the independence of the replication, its authors should not include any authors of the original study, nor their close collaborators.
\end{itemize}

A newly submitted replication is assigned to a member of the editorial board, which at this time is composed of 12~scientists from different research domains. The handling editor recruits two reviewers from a pool of currently 93 volunteers. The reviewing process consists of a dialog between the reviewers, the authors, and the handling editor whose goal is to improve the submission to the point that is can be accepted. In particular, the reviewers verify that they can reproduce the results from the supplied code and data, and judge if the replication claims made by the authors are valid subject to the criteria of their scientific domain. The entire reviewing process is openly conducted on the GitHub platform, meaning that contributions are open to read for anyone, and anyone with a GitHub account can participate by leaving a comment. Once the submission is deemed acceptable, it is added to the table of contents and to the ReScience archive, with an additional copy deposited on Zenodo~\cite{Zenodo} in order to ensure a more permanent archive than the GitHub platform can guarantee. Zenodo also issues a DOI that serves as a persistent reference.

The outstanding feature of this reviewing process, even compared to other journals practicing open peer review, is the rapid interaction between reviewers and authors that does not require the constant intervention of the handling editor. This rapid exchange has turned out to be essential in the quick resolution of the technical issues that inevitably arise when dealing with software and data.

Another outstanding feature of ReScience~C is its reliance on no other infrastructure than two digital platforms, GitHub and Zenodo, which are both free to use. Considering that editors and reviewers as well as authors are unpaid volunteers, this means that ReScience~C has so far been able to operate without any budget at all, and thereby avoid being subjected to any political pressure. We note however that this may not always be true for the individual volunteers contributing to ReScience~C because the open reviewing process provides no anonymity.


%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{rescience}
%
\end{document}
